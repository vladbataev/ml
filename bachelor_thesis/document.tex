\documentclass[pdftex,ptm,12pt,a4paper]{report}
\renewcommand{\baselinestretch}{1.5}
\setcounter{secnumdepth}{5}

% PDF search & cut'n'paste
\usepackage{cmap}
\usepackage[table,xcdraw]{xcolor}

% Cyrillic support
\usepackage{mathtext}
\usepackage{amsmath}
\usepackage[T1,T2A]{fontenc}
\DeclareSymbolFont{T2Aletters}{T2A}{cmr}{m}{it}
\usepackage[utf8]{inputenc}

\usepackage[bottom=20mm,top=20mm,right=20mm,left=30mm,headsep=0cm,nofoot]{geometry}

\usepackage{array}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\makeatletter
\renewcommand*{\ps@plain}{%
  \let\@mkboth\@gobbletwo
  \let\@oddhead\@empty
  \def\@oddfoot{%
    \reset@font
    \hfil
    \thepage
    % \hfil % removed for aligning to the right
  }%
  \let\@evenhead\@empty
  \let\@evenfoot\@oddfoot
}
\makeatother
\pagestyle{plain}

\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[russian,english]{babel}
    \addto{\captionsenglish}{\renewcommand{\bibname}{Литература}}
    \addto\captionsenglish{\renewcommand{\figurename}{Рис.}}
    \addto\captionsenglish{\renewcommand{\contentsname}{Содержание}}
    \addto\captionsenglish{\renewcommand{\proofname}{Доказательство}}
\usepackage{hyperref}
\usepackage{url}
\usepackage{abstract}
\usepackage{float}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\renewcommand*{\proofname}{Доказательство}
\usepackage{indentfirst}
\usepackage{color}
\usepackage{natbib}
\usepackage{bbm, dsfont}


% Detect whether PDFLaTeX is in use
\usepackage{ifpdf}

% Fix links to floats
\usepackage[all]{hypcap}

\makeatletter
\renewcommand{\@chapapp}{Часть}
\makeatother

% Theorem Styles
\newtheorem{theorem}{Теорема}[chapter]
\newtheorem{lemma}[theorem]{Лемма}
\newtheorem{claim}[theorem]{Теорема}
% Definition Styles
\theoremstyle{definition}
\newtheorem{definition}{Определение}[chapter]
\newtheorem{example}{Пример}[chapter]
% Rule for Title Page
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\usepackage{graphicx, epsfig}
\graphicspath{ {images/} }
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\begin{titlepage}
\newpage

\begin{center}
МИНИСТЕРСТВО ОБРАЗОВАНИЯ И НАУКИ РОССИЙСКОЙ ФЕДЕРАЦИИ \\
\vspace{0.5cm}
ГОСУДАРСТВЕННОЕ ОБРАЗОВАТЕЛЬНОЕ УЧРЕЖДЕНИЕ \\*
ВЫСШЕГО ПРОФЕССИОНАЛЬНОГО ОБРАЗОВАНИЯ\\*
"МОСКОВСКИЙ ФИЗИКО-ТЕХНИЧЕСКИЙ ИНСТИТУТ \\*
(ГОСУДАРСТВЕННЫЙ УНИВЕРСИТЕТ)" \\*
\vspace{0.5cm}
ФАКУЛЬТЕТ ИННОВАЦИЙ И ВЫСОКИХ ТЕХНОЛОГИЙ \\*
КАФЕДРА АНАЛИЗА ДАННЫХ \\*
\hrulefill
\end{center}


\vspace{8em}

\begin{center}
\Large Выпускная квалификационная работа по направлению 01.03.02 <<Прикладные математика и информатика>> \linebreak НА ТЕМУ:
\end{center}

\vspace{2.5em}

\begin{center}
\textsc{\large{\textbf{ТЕМАТИЧЕСКАЯ СЕГМЕНТАЦИЯ РАЗГОВОРОВ КОНТАКТНОГО ЦЕНТРА}}}
\end{center}

\vspace{6.5em}

\begin{flushleft}
Студент \hrulefill Батаев В.В. \\
\vspace{1.5em}
Научный руководитель д.ф-м.н \hrulefill Воронцов К.В.\\
\vspace{1.5em}
Зам. зав. кафедрой д.ф-м.н, профессор \hrulefill Бунина Е.И.
\end{flushleft}

\vspace{\fill}

\begin{center}
МОСКВА, 2017
\end{center}

\end{titlepage}

\tableofcontents

\sloppy

%\chapter{Введение}

\chapter{Тематическая сегментация текста}
\section{Постановка задачи}

Сформулируем формальную постановку задачи. Пусть дано предложение на естественном языке, которое можно считать последовательностью термов: $w_1, \dots w_n$. В качестве термов могут выступать отдельные слова, $n$-граммы, коллокации. Для каждого терма требуется поставить метку темы, то есть некоторой обобщающей данное слово сущности в зависимости от контекста. Таким образом решается задача $\emph{sequence labelling}$, то есть обучения отображения $f: W^k \rightarrow T^k$, $k \geq 0$, $W$, $T$ --- конечные множества. Подпоследовательности из одинаковых тем будем называть монотематичными $\emph{сегментами}$.  

\begin{figure}[t]
	\includegraphics[width=1.0\textwidth]{segmentation_example.png}
    \caption{Пример сегментации}
    \label{fig:awesome_image}
\end{figure}

\section{Метрики качества}

В предыдущих работах \cite{WD}\cite{Riedl_textsegmentation} для оценки качества сегментации использовалась метрика $WindowDiff$, которая рассчитывается по следующей формуле:
\[WindowDiff(R, C, k) = \frac{1}{N - k}\sum_{i=1}^{N-k}I(|R_{i, i + k} - C_{i, i + k}| > 0)
,\]
где $N$ --- число слов в предложении, $k$ --- ширина окна, $R_{i, i + k}$ --- эталонное число границ сегментов внутри окна $\{w_i, \dots w_{i + k}\}$, $C_{i, i + k}$ --- предполагаемое. Значение данной метрики показывает, сколько в среднем раз алгоритм неверно расставляет сегменты в окне ширины $k$. Обычно в качестве $k$ берут $\frac{N}{2 \cdot number\_of\_segments}$. Недостатком данной метрики является то, что она никак не учитывает, к каким темам были отнесены получившиеся сегменты.

Чтобы обойти выше определенную проблему будем мерить усредненный взвешенный $f1-score$ и $accuracy$ для каждой темы по всем словам из всех предложений корпуса. В таком случае метрика качества сегментации фокусируется на отдельных словах и на точном проведении границ сегментов. Однако, если слово из одной темы окажется в середине сегмента другой темы или ее конце, то метрика оштрафует эти оба случая одинаково.  

Если же не важно знать точные границы сегментов, а достаточно получить только порядок следования тем, то метрикой качества может быть редакторское расстояние между последовательностями тем сегментов.  
 

\section{Обзор существующих методов}\label{old}

При решении задачи тематической сегментации текста существует два принципиально различных подхода. Первый основан на unsupervised learning, не требующей размеченных отсегментированных текстов, в котором основной идеей является предположение, что граница между тематическими сегментами проходит при резкой смене темы локального контекста.

В работе \cite{Hearst} использовался алгоритм, основанный на движении скользящего окна по тексту. Для каждого положения окна строится bag-of-words вектор, сглаженный с помощью tf-idf по некоторой большой коллекции документов. По построенным векторам строилась последовательность косинусных расстояний между соседними окнами, и в ней точки максимума или точки со значениями, большими некоторого заранее определенного порога, определяются как границы сегментов. Однако же данный метод не позволяет определить темы построенных сегментов. 

В работе \cite{Riedl_textsegmentation} по корпусу строится вероятностная тематическая LDA модель (\cite{Blei:2003:LDA:944919.944937}), с помощью которой для каждого слова в предложении имеется распределение по темам. Затем векторы распределений тем соседних слов сравниваются с помощью косинусной метрики и производится расстановка сегментов аналогично методу со скользящим окном.  

Второй подход основан на supervised learning, использующий разметку текста на сегменты. В работе \cite{Wang2016} предлагается решать задачу классификации для каждого предложения --- поставить границу сегмента после данного предложения или нет, используя нейросетевую архитектуру, состоящую из рекуррентных и сверточных слоев, которая извлекает векторное представление предложения невысокой размерностью по сравнению с размером словаря. Отметим, что в данной работе также нет прямой возможности узнать, к каким темам относятся построенные сегменты.

Все выше описанные подходы применялись для документов сравнительно большой длины, для которых описания их структуры через входящие в них слова являются достаточно статистически значимыми. В данной же работе основной проблемой является то, что объектами сегментации являются отдельные слова, для которых сложно строить признаки, основанные на большом контексте. 

\chapter{Описание новых предложенных методов}

Будем считать, что перед нами стоит supervised задача, поскольку качество ее решения окажется заведомо выше, нежели в unsupervised случае. Первым шагом всех ниже описанных методов будет построение тематической модели по всем документам коллекции с использованием априорных знаний о структуре некоторых тем. Поэтому перейдем к подробному описанию использованной модели.

\section{Вероятностное тематическое моделирование}

Пусть $D$ --- множество текстовых документов в коллекции, $W$ --- множество слов коллекции, $T$ --- множество латентных переменных, именуемых темами.

Для каждого документа $d \in D$ известен вектор частот $n_{dw}$ токена $w\in W$.  Необходимо найти условные распределения $p(w|t)$ частоты слова $w$ для темы $t$ и условное распределение $p(t|d)$ частоты темы $t$ в документе $d$. 

Введем следующие обозначения: $\theta_{td} = p(t|d), \phi_{wt} = p(w|t). $   

Для вывода в тематической модели примем гипотезу условной независимости:
\[
	 p(w|d,t) = p(w|t), ~ w \in W,
\]
то есть токены документа определяются только тематикой.

Коллекция документов рассматривается как случайная и независимая выборка
троек ($w_i$, $d_i$, $t_i$), $i = 1,..., n$ из дискретного распределения $p(w, d, t)$ на конечном вероятностном пространстве $W \times D \times T$.

В силу данных предпололжений справедливо соотношение:
\[p(w|d) = \sum_{t\in T}p(t|d)p(w|t), ~ w \in W^{m}, d\in D.\]

Параметры модели образуют матрицы $\Phi = (\phi_{wt})_{W \times T}$ и $\Theta=(\theta_{td})_{T \times D}$.


Оценки латентных параметров будем искать с помощью EM-алгоритма, максимизируя регуляризованный \cite{Vorontsov2015} логарифм правдоподобия:

\[ L(\Phi, \Theta) + R(\Phi, \Theta) = \ln \prod_{i = 1}^{n}p(w_i|d_i) + R(\Phi, \Theta) = \sum_{d\in D}\sum_{w\in W}n_{dw}\ln p(w|d) + R(\Phi, \Theta) \rightarrow  \max_{\Phi, \Theta} \]

На матрицы $\Phi, \Theta$ наложены естественные ограничения неотрицательности и нормированности столбцов.

Таким образом для нахождения оценки параметров модели $\Phi, \Theta$ решается следующая оптимизационная задача:

\[\Phi^{*}, \Theta^{*} = \argmax_{\Phi, \Theta} L(\Phi, \Theta ) + R(\Phi, \Theta) ,\]
\[
\sum_{w\in W^{m}} \phi_{wt} = 1;~ \phi_{wt} \geq 0; 
\]
\[
\sum_{t\in T} \theta_{td} = 1;~ \theta_{td} \geq 0. 
\]

Регуляризатор $R(\Phi, \Theta)$ отвечает за ограничения, которые могут быть наложены на модель. Такими ограничениями могут быть, например, принадлежность лишь небольшого числа слов к конкретной теме, либо близость векторов тем соседних слов в предложении. Опишем использованные в данной работе регуляризаторы.

Пусть $\alpha_{td} = p(t|d), \beta_{wt} = p(w|t)$ --- произвольные распределения.
\[R(\Phi, \Theta) = -\beta_0\sum_{t \in T}\sum_{w\in W}\beta_{wt}\ln\phi_{wt} - \alpha_0\sum_{d \in D}\sum_{t\in T}\alpha_{td}\ln\theta_{td} \rightarrow \max - \text{регуляризатор разреживания} \]

Данный регуляризатор максимизирует расстояние Кульбака-Лейблера между равномерными дискретнымм распределениями и распределениями, заданными в параметрах $\alpha_{td}$ и $\beta_{wt}$. Коэффициенты $\alpha_0$ и $\beta_0$ отвечают за баланс между разреживанием соответствующих распределений и максимизацией правдоподобия.

Для повышения качества тематической модели будем использовать частичное обучение.
Пусть $D_0 \subset D$ --- подмножество размеченных документов, и для каждого документа $d \in D_0$ задано подмножество релевантных тем $T_d \subset T$, к которым он относится,
и подмножество нерелевантных тем $\overline{T}_d \subset T$, к которым он не относится. 
Инициализируем параметры $\alpha_{td} = -\frac{1}{T_d}[t \in \overline{T}_d][d \in D_0]$ разреживающего регуляризатора --- получим регуляризатор частичного обучения. При достаточно большом значении коэффициента регуляризации $\alpha_0$ регуляризатор разреживания обнуляет вероятности нерелевантных тем в документах и токенов в темах. 


 Построим semi-supervised вероятностную тематическую модель, использующую априорные знания о принадлежности некоторых документов к определенным темам. Выход тематической модели задает некоторое приближение для сегментации, поскольку тему слова $w_k$ можно определить как $y_k = \argmax_{t\in T}p(t|w_k,d)$, но векторы тем слов не учитывают контекст напрямую. Для решения данной проблемы предлагаются следующие методы: 
\begin{itemize}
	\item Сглаживать векторы тем соседних слов с помощью векторов из окна фиксированной ширины
	
	\item Ввести дополнительную регуляризацию тематической модели на близость векторов тем соседних слов
	
	\item Моделировать взаимосвязи между темами слов предложения с помощью дискриминативной модели, а именно CRF
	
\end{itemize} 

\section{Сглаживание векторов тем соседних слов}\label{smooth}

Напомним, что результатом алгоритма тематического моделирования являются распределения $\phi_{wt} = p(w|t)$, $\theta_{td} = p(t|d)$. Из формулы Байеса и гипотезы условной независимости слова при данном документе и теме следует:
\[p(t|w, d) = \frac{p(t,w|d)}{p(w|d)} = \frac{p(w|t)p(t|d)}{p(w|d)} = \frac{\phi_{wt}\theta_{td}}{\sum_{t \in T}\phi_{wt}\theta_{td}}.
\]

Сглаживание будем производить итеративно последовательно по всем словам документа по следующей формуле:
\[g(t,d ,k,i) = f(i)p(t|d, w_{k + i})\]
\[p^{*}(t|d, w_k) = norm_{t\in T}(\sum_{j=-h, j \ne 0}^{h}g(t,d,k,j))
\] 
$f(i)$ --- функция затухания влияния контекста на распределение текущего слова, $h$ --- размер окна. Число итераций для документов --- параметр модели.


\section{Условное случайное поле} \label{crf}
В задаче тематической сегментации распределение тем при условии данного слова и документа не учитывает влияния контекста внутри предложения. Для решения данной проблемы предлагается использовать модель условного случайного поля или CRF для моделирования зависимостей между темами разных слов. Преимущество CRF в том, что такая модель является дискриминативной, а следовательно способна более точно моделировать условное распределение $p(\boldsymbol{y}|\boldsymbol{x})$ неизвестной последовательности меток $\boldsymbol{y}$ при условии обозреваемой последовательности слов $\boldsymbol{x}$.  

В данной задаче будем использовать linear-chain-CRF, чей граф представляет собой линейную цепь. В таком случае условное распределение можно записать в следующем виде:
\[
p(\boldsymbol{y}|\boldsymbol{x}) = \frac{1}{Z(\boldsymbol{x})} \prod_{t=1}^{T} \exp{\{
	\sum_{k=1}^{K}\theta_{k}f_k(y_t, y_{t-1}, \boldsymbol{x}_t)
	\}},
\]
где $Z(\boldsymbol{x})$, зависящая от конкретного $\boldsymbol{x}$ функция нормализации
\[
Z(\boldsymbol{x}) = \sum_{\boldsymbol{y}}\prod_{t=1}^{T}\exp{ \{
	\sum_{k=1}^{K}\theta_{k}f_k(y_t, y_{t-1}, \boldsymbol{x}_t) \}
}.
\]
\begin{figure}[t]
	\begin{center}
		\includegraphics[width=0.6\textwidth]{crf.png}
		\caption{Графическая модель linear chain CRF, в которой переходные факторы зависят от всей обозреваемой последовательности}
		\label{fig:crf_image}
	\end{center}
\end{figure}

$f_k(y, y'\boldsymbol{x}_t)$ --- произвольные вещественнозначные функции, которые также называются $\emph{feature functions}.$ Они описывают зависимости между соседними метками тем и произвольным подмножеством входных переменных из последовательности. Подчеркиваем, что $\boldsymbol{x}_t$ может зависеть от произвольного подмножества $\{x_1, \dots x_T\}$. Таким образом условное распределение $p(\boldsymbol{y}|\boldsymbol{x})$ не накладывает каких-либо ограничений на $p(\boldsymbol{x})$.

В качестве $f_k$ будем использовать:
\begin{itemize} 
	\item Вектор $p(t|w,d)$ из обученной предварительно semi-supervised тематической модели
	\item Их сглаженные версии после итеративного алгоритма, описанного в \ref{smooth}
	\item word2vec представление слова $w_t$
	\item Косинусные расстояния между word2vec представлениями словом $w_t$ и словами некоторого окна $w_{t-h}, \dots w_{t+h}$
\end{itemize}

TODO: вывод в linear-chain CRF, обучение параметров $\theta_k$ 


\chapter{Результаты экспериментов}

\section{Входные данные}

Обучающая выборка состоит из 400 диалогов контактного центра, в которых были оставлены только реплики оператора. В каждой такой реплике проставлены границы сегментов, и каждый из сегментов отнесен к некоторой теме. Число тем было выбрано равным 53 экспертом. 

Для построения тематической модели с использованием частичного обучения использовалось open-source библиотека BigArtm \cite{bigartm}. Для каждой темы была размечена небольшая группа до десяти документов, относящихся к данной теме. Регуляризатор частичного обучения запускался на одну итерацию с коэффициентом $\alpha_0$, равным 100. 

TODO: привести примеры тем и сегментов

\section{Сглаживание векторов тем}

Для эксперимента, описанного в \ref{smooth}, была взята функция $f(i) = \frac{1}{i^2}$. Тема каждого слова определялась как $\argmax_{t\in T}p(t|w,d)$.  Покажем зависимость качества сегментации от числа итераций и размера окна на графиках \ref{fig:smoothing3}, \ref{fig:smoothing5}, \ref{fig:smoothing7}.
 
\begin{figure}
	\begin{center}
		\includegraphics[width=0.8\textwidth]{smoothing3.png}
		\caption{Размер окна 3}
		\label{fig:smoothing3}
		\includegraphics[width=0.8\textwidth]{smoothing5.png}
		\caption{Размер окна 5}
		\label{fig:smoothing5}
		\includegraphics[width=0.8\textwidth]{smoothing7.png}
		\caption{Размер окна 7}
		\label{fig:smoothing7}
		
	\end{center}
\end{figure}


\section{Метод, основанный на CRF}

В качестве feature functions были взяты всевозможные комбинации, описанные в \ref{crf}. Для получения сглаженных версий векторов $p(t|d,w)$ были взяты размер окна 5 и число итераций 3, поскольку они давали наилучшее качество сегментации по принципу аргмаксимума. Для оценки качества алгоритма использовалась 5-fold кросс-валидация.
Результаты представлены в таблице \ref{tab:CRF}:

\begin{table}
	\caption{\label{tab:CRF}Точность сегментации CRF}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			Feature functions & accuracy & weighted f1 & Edit distance & WD \\ 
			\hline
			topics & 0.564 & 0.541 & 0.444 & 0.104 \\ 			
			\hline
			w2v features & 0.601 & 0.591 & 0.381 & 0.138 \\
			\hline
			topics + w2v features & 0.620 & 0.615 & 0.358 & 0.136 \\  
			\hline
			smoothed topics & 0.521 & 0.496 & 0.471 & 0.117 \\
			\hline
			smoothed topics + w2v features & 0.634 & 0.629 & 0.361 & 0.145 \\
			\hline
			
		\end{tabular}
	\end{center}
\end{table}

\chapter{Заключение}

TODO: выводы

\iffalse

\section{Классификация синтаксических $n$-грамм}

Для каждой из синтаксических веток был построен вектор размерности 2400, полученный из модель skip-thought, построенной по текстам чатов технической поддержки, состоящий из более 40 миллиона слов. Тематика чатов несколько отличается от тематики текстов разговоров, тем не менее в них присутствует банковская лексика. Каждая из $n$-грамм имеет метку темы. Всего имеется 40 тем, полученных из асессорской разметки. Таким образом решается задача многоклассовой классификации на 40 классов. Лучшие результаты показала логистическая регрессия с $l_{2}$--регуляризацией: взвешенный f1-score 0.58 при 3-fold кросс-валидации.    
\fi
\bibliography{literature}
\bibliographystyle{plain}

\end{document}\grid
