{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pycrfsuite\n",
    "import sklearn_crfsuite\n",
    "import gensim\n",
    "import re\n",
    "\n",
    "from collections import namedtuple\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Маппинг тем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "translation = [27, 37, 19, [3, 16, 33], 11, 25, 21, 6, 29, [13, 33], 17, 25, 15, 22, 28, 41, 18, 40, 23, [7, 8], 31,\n",
    " 20, 17, 32, 25, 38, 14, 25, 36, 25, 4, 24, 5, [7, 8], 25, 12, 9, 25, 24, 26, 30, 35, 13, 3,\n",
    " 1, 14, 2, 17, 10, 0, 5, 21, 12, -1]\n",
    "translation[27] = [25, 42, 34, 33, 12]\n",
    "translation[43] = [3, 34]\n",
    "translation[22] = [17, 18]\n",
    "translation[47] = [17, 38]\n",
    "translation[12] = [15, 27]\n",
    "translation[43] = [3, 34, 12]\n",
    "translation[21] = [20, 12]\n",
    "translation[25] = [38, 12, 25]\n",
    "translation[42] = [13, 16]\n",
    "translation[10] = [17, 38]\n",
    "translation[24] = [25, 37]\n",
    "del translation[47]\n",
    "translation_set = []\n",
    "for element in translation:\n",
    "    try:\n",
    "        translation_set.append(set(element))\n",
    "    except:\n",
    "        translation_set.append({element})\n",
    "translation = translation_set[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{37}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mystem = Mystem()\n",
    "\n",
    "def stem(s):\n",
    "    return [(e['text'].strip(), \\\n",
    "             e['analysis'][0]['lex'] \\\n",
    "                 if 'analysis' in e and len(e['analysis']) > 0 else '', \\\n",
    "             re.match('^([A-Z]+)', e['analysis'][0]['gr']).group(0) \\\n",
    "                 if 'analysis' in e and len(e['analysis']) > 0 else '', \\\n",
    "             ','.join(set(re.findall(r\"[\\w']+\", e['analysis'][0]['gr'])[1:])) \\\n",
    "                 if 'analysis' in e and len(e['analysis']) > 0 else '')\\\n",
    "             for e in mystem.analyze(s) if len(e['text'].strip()) > 0]\n",
    "\n",
    "def get_pos_tag(word):\n",
    "    temp = stem(word)[0]\n",
    "    return temp[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"./data/vw.txt\") as fin:\n",
    "    text = fin.read()\n",
    "topics = pd.read_pickle(\"./data/topics_assessors.pkl\")\n",
    "ptdw = pd.read_pickle(\"./data/ptdw\")\n",
    "w2v_model = gensim.models.Word2Vec.load(\"./data/transcriptions_w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"банк_S\" in w2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Их предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "phrases, topic_probs, labels = [], [], []\n",
    "for index, line in enumerate(text.split(\"\\n\")):\n",
    "    temp = line.split()\n",
    "    phrase_id = temp[0]\n",
    "    phrase = temp[2:]\n",
    "    probs = ptdw[phrase_id].as_matrix()\n",
    "    phrase_size = 0\n",
    "    if len(probs.shape) < 2:\n",
    "        phrase_size = 1\n",
    "        probs = np.expand_dims(probs, axis=1)\n",
    "    else:\n",
    "        phrase_size = probs.shape[1]\n",
    "    if phrase_size != len(phrase):\n",
    "        raise\n",
    "    if phrase_id in topics:\n",
    "        phrases.append(phrase)\n",
    "        topic_probs.append(probs)\n",
    "        labels.append(topics[phrase_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def argmax_topic_segmentation(topic_prob):\n",
    "    return [np.argmax(topic_prob[:, i]) for i in range(topic_prob.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{2}, {22}, {12}, {27}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[translation[i] for i in argmax_topic_segmentation(topic_probs[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23a']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"23a\".split(\"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Генерация фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def meta_word2vec(meta_word, w2v_model):\n",
    "    meta_word_vector = np.zeros(300)\n",
    "    length = 0\n",
    "    for w in meta_word.split(\"_\"):\n",
    "        with_pos = w + \"_\" + get_pos_tag(w)\n",
    "        if with_pos in w2v_model:\n",
    "            meta_word_vector += w2v_model[with_pos]\n",
    "            length += 1\n",
    "    if length == 0:\n",
    "        return None\n",
    "    return meta_word_vector / length\n",
    "    \n",
    "def distance_vector(phrase, index, w2v_model):\n",
    "    source_vector = meta_word2vec(phrase[index], w2v_model)\n",
    "    distances = []\n",
    "    for word in phrase:\n",
    "        distances.append(cosine(metaword2vec(word), source_vector))\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def phrase_to_features(phrase, topic_prob=None, w2v_model=None):\n",
    "    features = []\n",
    "    for word_index, word in enumerate(phrase):\n",
    "        word_features = {}\n",
    "        word_features[\"word\"] = word\n",
    "        if topic_prob is not None:\n",
    "            for t in range(topic_prob.shape[0]):\n",
    "                word_features[\"topic {}\".format(t)] = topic_prob[t][word_index]\n",
    "        if w2v_model is not None:\n",
    "            meta_word_vector = meta_word2vec(word, w2v_model)\n",
    "#             for i in range(len(meta_word_vector)):\n",
    "#                 word_features[\"w2v {}\".format(i)] = meta_word_vector[i]\n",
    "            try:\n",
    "                if meta_word_vector is not None:\n",
    "                    if word_index > 0:\n",
    "                        left_word = meta_word2vec(phrase[word_index - 1], w2v_model)\n",
    "                        if left_word is not None:\n",
    "                            word_features[\"-1 w2v\"] = cosine(left_word, meta_word_vector)\n",
    "                    if word_index < len(phrase) - 1:\n",
    "                        right_word = meta_word2vec(phrase[word_index + 1], w2v_model)\n",
    "                        if right_word is not None:\n",
    "                            word_features[\"+1 w2v\"] = cosine(right_word, meta_word_vector)\n",
    "            except:\n",
    "                print(phrase)\n",
    "                print(word, word_index)\n",
    "                raise\n",
    "        features.append(word_features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "5-fold кросс валидация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Param = namedtuple(\"Param\", field_names=[\"name\", \"value\"])\n",
    "\n",
    "params = {\n",
    "    \"topics\": [#Param(\"Topics disabled\", [None for i in range(len(phrases))]), \n",
    "               Param(\"Topics enabled\", topic_probs)],\n",
    "    \"w2v\": [#Param(\"w2v disabled\", None), \n",
    "            Param(\"w2v enabled\", w2v_model)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic param: Topics enabled, w2v_param : w2v enabled --- 0.5870079509871325\n",
      "CPU times: user 53.7 s, sys: 144 ms, total: 53.8 s\n",
      "Wall time: 54.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params_scores = {}\n",
    "rs = ShuffleSplit(n_splits=5)\n",
    "splits_indexes = list(rs.split(list(range(len(phrases)))))\n",
    "for topic_param in params[\"topics\"]:\n",
    "    rs = ShuffleSplit(n_splits=5)\n",
    "    for w2v_param in params[\"w2v\"]:\n",
    "        X_data = [phrase_to_features(phrases[i], topic_param.value[i], w2v_param.value) for i in range(len(phrases))]\n",
    "        y_data = [[str(x) for x in labels[i]] for i in range(len(labels))]\n",
    "        scores = []\n",
    "        for train_indexes, test_indexes in splits_indexes:\n",
    "            X_train, X_test, y_train, y_test = [], [], [], []\n",
    "            for train_index in train_indexes:\n",
    "                X_train.append(X_data[train_index])\n",
    "                y_train.append(y_data[train_index])\n",
    "            for test_index in test_indexes:\n",
    "                X_test.append(X_data[test_index])\n",
    "                y_test.append(y_data[test_index])\n",
    "            crf = sklearn_crfsuite.CRF(algorithm=\"lbfgs\", max_iterations=100, \n",
    "                                   all_possible_transitions=True, all_possible_states=True)\n",
    "            crf.fit(X_train, y_train)\n",
    "            y_pred = crf.predict(X_test)\n",
    "            scores.append(metrics.flat_f1_score(y_test, y_pred, average=\"weighted\"))\n",
    "        score = np.mean(scores)\n",
    "        print(\"Topic param: {}, w2v_param : {} --- {}\".format(topic_param.name, w2v_param.name, score))\n",
    "        params_scores[(topic_param.name, w2v_param.name)] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59124186393515565"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
