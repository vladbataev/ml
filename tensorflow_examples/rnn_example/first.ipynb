{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import functools\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/gpu:0', '/gpu:1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lazy_property(function):\n",
    "    attribute = '_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def wrapper(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RnnClassifierModel:\n",
    "    def __init__(self, data, target, vocab_size,\n",
    "                 dropout=0.5, num_hidden=100, num_layers=2, emb_dim=256, num_classes=2):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.dropout = dropout\n",
    "        self._num_layers = num_layers\n",
    "        self._num_hidden = num_hidden\n",
    "        self._num_layers = num_layers\n",
    "        self._emb_dim = emb_dim\n",
    "        self._vocab_size = vocab_size\n",
    "        self._num_classes = num_classes\n",
    "        \n",
    "    @staticmethod\n",
    "    def _weight_and_bias(in_size, out_size):\n",
    "        weight = tf.truncated_normal([in_size, out_size], stddev=0.01)\n",
    "        bias = tf.constant(0.1, shape=[out_size])\n",
    "        return tf.Variable(weight), tf.Variable(bias)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _embedding_matrix(vocab_size, emb_dim):\n",
    "        return tf.Variable(tf.random_uniform([vocab_size + 1, emb_dim]), name=\"embedding\")\n",
    "        \n",
    "    @lazy_property\n",
    "    def hidden_states(self):\n",
    "        embedding_matrix = self._embedding_matrix(self._vocab_size, self._emb_dim)\n",
    "        emb_vector = tf.nn.embedding_lookup(embedding_matrix, self.data)\n",
    "        network = tf.nn.rnn_cell.GRUCell(self._num_hidden)\n",
    "        network = tf.nn.rnn_cell.DropoutWrapper(\n",
    "            network, output_keep_prob=self.dropout)\n",
    "        network = tf.nn.rnn_cell.MultiRNNCell([network] * self._num_layers)\n",
    "        output, _ = tf.nn.dynamic_rnn(network, emb_vector, dtype=tf.float32, swap_memory=True)\n",
    "        # Select last output.\n",
    "        output = tf.transpose(output, [1, 0, 2])\n",
    "        return output\n",
    "        \n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        output = self.hidden_states\n",
    "        last = output[-1, :, :]\n",
    "        # Softmax layer.\n",
    "        softmax_weights, softmax_bias = self._weight_and_bias(self._num_hidden, self._num_classes)\n",
    "        probs = tf.nn.softmax(tf.matmul(last, softmax_weights) + softmax_bias)\n",
    "        return probs\n",
    "    \n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        probs = self.prediction\n",
    "        logits = tf.log(probs)\n",
    "        return tf.nn.sparse_softmax_cross_entropy_with_logits(logits, self.target)\n",
    "\n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        return optimizer.minimize(self.cost)\n",
    "\n",
    "    @lazy_property\n",
    "    def error(self):\n",
    "        mistakes = tf.not_equal(\n",
    "            tf.cast(self.target, tf.int64), tf.argmax(self.prediction, 1))\n",
    "        return tf.reduce_mean(tf.cast(mistakes, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dictionary(filenames):\n",
    "    index = 1\n",
    "    dictionary = {}\n",
    "\n",
    "    for filename in filenames:\n",
    "        with open(filename) as in_file:\n",
    "            for row in in_file:\n",
    "                words = (row.split(',')[1]).split()\n",
    "                for word in words:\n",
    "                    if word not in dictionary:\n",
    "                        dictionary[word] = index\n",
    "                        index += 1\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63609 ./train_data/train.csv\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l ./train_data/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filenames = glob.glob(\"./train_data/*csv\")\n",
    "dictionary = build_dictionary(filenames)\n",
    "embedding_dimension = 256\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 256\n",
    "num_objects = 63609\n",
    "\n",
    "def preprocess(example, table):\n",
    "    return table.lookup(tf.string_split(example).values)\n",
    "    \n",
    "def read_my_file_format(filename_queue):\n",
    "    reader = tf.TextLineReader()\n",
    "    key, record_string = reader.read(filename_queue)\n",
    "    record_defaults = [[1], [''], [1]]\n",
    "    dialog_id, dialog, label = tf.decode_csv(record_string, record_defaults=record_defaults)\n",
    "    dialog = tf.pack([dialog])\n",
    "    processed_example = preprocess(dialog, table)\n",
    "    return processed_example, label\n",
    "\n",
    "def input_pipeline(filenames, batch_size, num_epochs=None):\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        filenames, num_epochs=num_epochs, shuffle=True)\n",
    "    example, label = read_my_file_format(filename_queue)\n",
    "    capacity = 100\n",
    "    example_batch, label_batch = tf.train.batch(\n",
    "        [example, label], \n",
    "        batch_size=batch_size, \n",
    "        capacity=capacity,\n",
    "        dynamic_pad=True,\n",
    "        allow_smaller_final_batch=True,\n",
    "    )\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13719\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch roc-auc: 0.5014070224141467\n",
      "Epoch 0 is finished for 3.2983884811401367 sec\n",
      "Epoch roc-auc: 0.49635059830778316\n",
      "Epoch 1 is finished for 3.6774682998657227 sec\n",
      "Epoch roc-auc: 0.4967166411467241\n",
      "Epoch 2 is finished for 4.512049198150635 sec\n",
      "Epoch roc-auc: 0.49811618703898247\n",
      "Epoch 3 is finished for 3.7752153873443604 sec\n",
      "Epoch roc-auc: 0.4987432818720713\n",
      "Epoch 4 is finished for 3.421198844909668 sec\n",
      "Epoch roc-auc: 0.49875175043347075\n",
      "Epoch 5 is finished for 3.034984588623047 sec\n",
      "Epoch roc-auc: 0.49473489477195565\n",
      "Epoch 6 is finished for 5.296198606491089 sec\n",
      "Epoch roc-auc: 0.5011181410470276\n",
      "Epoch 7 is finished for 6.576495170593262 sec\n",
      "Epoch roc-auc: 0.4981695725994325\n",
      "Epoch 8 is finished for 3.0298824310302734 sec\n",
      "Epoch roc-auc: 0.4995887668907453\n",
      "Epoch 12 is finished for 2.973670721054077 sec\n"
     ]
    }
   ],
   "source": [
    "keys = tf.constant(list(dictionary.keys()))\n",
    "values = tf.constant(list(dictionary.values()), dtype=tf.int64)\n",
    "table = tf.contrib.lookup.HashTable(\n",
    "            tf.contrib.lookup.KeyValueTensorInitializer(keys, values), -1)\n",
    "\n",
    "inputs, target = input_pipeline(filenames, batch_size=batch_size)\n",
    "\n",
    "with tf.device(\"/gpu:1\"):\n",
    "    model = RnnClassifierModel(inputs, target, vocab_size=len(dictionary), num_classes=2)\n",
    "    training_opt = model.optimize\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    table.init.run()\n",
    "    sess.run(init_op)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        epoch_prediction = np.array([])\n",
    "        epoch_true_targets = np.array([])\n",
    "        for _ in range(num_objects // batch_size + 1):\n",
    "            start_time = time.time()\n",
    "            sess.run(training_opt)\n",
    "            batch_prediction = sess.run(model.prediction)[:, 1]\n",
    "            batch_true_target = sess.run(target)\n",
    "            epoch_prediction = np.concatenate([epoch_prediction, batch_prediction], axis=0)\n",
    "            epoch_true_targets = np.concatenate([epoch_true_targets, batch_true_target], axis=0)\n",
    "            #print(\"Batch error: {}\".format(batch_error))\n",
    "            #print(\"Batch running time: {}\".format(time.time() - start_time))\n",
    "            #epoch_error.append(batch_error)\n",
    "        print(\"Epoch roc-auc: {}\".format(roc_auc_score(epoch_true_targets, epoch_prediction)))\n",
    "        print(\"Epoch {} is finished for {} sec\".format(i, time.time() - start_time))\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    save_path = saver.save(sess, \"./tmp/model.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('ads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kek\n"
     ]
    }
   ],
   "source": [
    "print(\"kek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_X = tf.placeholder(dtype=tf.int64, shape=[1, None])\n",
    "\n",
    "inputs, _ = input_pipeline([\"./data/file0.csv\", \"./data/file1.csv\"], \n",
    "                                batch_size=batch_size, )\n",
    "\n",
    "model.data = inputs\n",
    "prediction_op = model.prediction\n",
    "saver = tf.train.Saver()\n",
    "ckpt = tf.train.get_checkpoint_state(\"./tmp/\")\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    #sess.run(init_op)\n",
    "    table.init.run()\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    res = sess.run(prediction_op, feed_dict={test_X: [[1, 2]]})\n",
    "    print(res)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
